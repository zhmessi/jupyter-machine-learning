{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in-graph模式和between-graph模式都支持同步和异步更新。\n",
    "\n",
    "在同步更新的时， 每次梯度更新，都要等所有分发出去的数据计算完成后，返回结果之后，把梯度累加算了均值之后，再更新参数。 这样的好处是loss的下降比较稳定， 但是这个的坏处也很明显， 处理的速度取决于最慢的那个分片计算的时间。\n",
    "\n",
    "在异步更新时， 所有的计算节点，各自算自己的， 更新参数也是自己更新自己计算的结果， 这样的优点就是计算速度快， 计算资源能得到充分利用，但是缺点是loss的下降不稳定， 抖动大。\n",
    "\n",
    "在数据量较小的情况下， 各个节点的计算能力比较均衡， 推荐使用同步模式；在数据量很大情况下，各个主机的计算性能掺差不齐的情况下，推荐使用异步的方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "运行步骤\n",
    "\n",
    "ps节点执行：\n",
    "\n",
    "python distributed.py --job_name=ps --task_index=0\n",
    "\n",
    "worker1节点执行：\n",
    "\n",
    "python distributed.py --job_name=worker --task_index=0\n",
    "\n",
    "worker2 节点执行：\n",
    "\n",
    "python distributed.py --job_name=worker --task_index=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding:utf-8\n",
    "import math\n",
    "import tempfile\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "IMAGE_PIEXLS = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义默认训练参数和数据路径\n",
    "flags.DEFINE_string('data_dir', '/tmp/mnist-data', 'Directory  for storing mnist data')\n",
    "flags.DEFINE_integer('hidden_units', 100, 'Number of units in the hidden layer of the NN')\n",
    "flags.DEFINE_integer('train_steps', 10000, 'Number of training steps to perform')\n",
    "flags.DEFINE_integer('batch_size', 100, 'Training batch size ')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Learning rate')\n",
    "# 定义分布式参数\n",
    "# 参数服务器parameter server节点\n",
    "flags.DEFINE_string('ps_hosts', '192.168.32.145:22221', 'Comma-separated list of hostname:port pairs')\n",
    "# 两个worker节点\n",
    "flags.DEFINE_string('worker_hosts', '192.168.32.146:22221,192.168.32.160:22221',\n",
    "                    'Comma-separated list of hostname:port pairs')\n",
    "# 设置job name参数\n",
    "flags.DEFINE_string('job_name', None, 'job name: worker or ps')\n",
    "# 设置任务的索引\n",
    "flags.DEFINE_integer('task_index', None, 'Index of task within the job')\n",
    "# 选择异步并行，同步并行\n",
    "flags.DEFINE_integer(\"issync\", None, \"是否采用分布式的同步模式，1表示同步模式，0表示异步模式\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "\n",
    "    if FLAGS.job_name is None or FLAGS.job_name == '':\n",
    "        raise ValueError('Must specify an explicit job_name !')\n",
    "    else:\n",
    "        print ('job_name : %s' % FLAGS.job_name)\n",
    "    if FLAGS.task_index is None or FLAGS.task_index == '':\n",
    "        raise ValueError('Must specify an explicit task_index!')\n",
    "    else:\n",
    "        print ('task_index : %d' % FLAGS.task_index)\n",
    "\n",
    "    ps_spec = FLAGS.ps_hosts.split(',')\n",
    "    worker_spec = FLAGS.worker_hosts.split(',')\n",
    "\n",
    "    # 创建集群\n",
    "    num_worker = len(worker_spec)\n",
    "    cluster = tf.train.ClusterSpec({'ps': ps_spec, 'worker': worker_spec})\n",
    "    server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\n",
    "    if FLAGS.job_name == 'ps':\n",
    "        server.join()\n",
    "\n",
    "    is_chief = (FLAGS.task_index == 0)\n",
    "    # worker_device = '/job:worker/task%d/cpu:0' % FLAGS.task_index\n",
    "    with tf.device(tf.train.replica_device_setter(\n",
    "            cluster=cluster\n",
    "    )):\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)  # 创建纪录全局训练步数变量\n",
    "\n",
    "        hid_w = tf.Variable(tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, FLAGS.hidden_units],\n",
    "                                                stddev=1.0 / IMAGE_PIXELS), name='hid_w')\n",
    "        hid_b = tf.Variable(tf.zeros([FLAGS.hidden_units]), name='hid_b')\n",
    "\n",
    "        sm_w = tf.Variable(tf.truncated_normal([FLAGS.hidden_units, 10],\n",
    "                                               stddev=1.0 / math.sqrt(FLAGS.hidden_units)), name='sm_w')\n",
    "        sm_b = tf.Variable(tf.zeros([10]), name='sm_b')\n",
    "\n",
    "        x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n",
    "        hid = tf.nn.relu(hid_lin)\n",
    "\n",
    "        y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n",
    "        cross_entropy = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "\n",
    "        train_step = opt.minimize(cross_entropy, global_step=global_step)\n",
    "        # 生成本地的参数初始化操作init_op\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        train_dir = tempfile.mkdtemp()\n",
    "        sv = tf.train.Supervisor(is_chief=is_chief, logdir=train_dir, init_op=init_op, recovery_wait_secs=1,\n",
    "                                 global_step=global_step)\n",
    "\n",
    "        if is_chief:\n",
    "            print ('Worker %d: Initailizing session...' % FLAGS.task_index)\n",
    "        else:\n",
    "            print ('Worker %d: Waiting for session to be initaialized...' % FLAGS.task_index)\n",
    "        sess = sv.prepare_or_wait_for_session(server.target)\n",
    "        print ('Worker %d: Session initialization  complete.' % FLAGS.task_index)\n",
    "\n",
    "        time_begin = time.time()\n",
    "        print ('Traing begins @ %f' % time_begin)\n",
    "\n",
    "        local_step = 0\n",
    "        while True:\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(FLAGS.batch_size)\n",
    "            train_feed = {x: batch_xs, y_: batch_ys}\n",
    "\n",
    "            _, step = sess.run([train_step, global_step], feed_dict=train_feed)\n",
    "            local_step += 1\n",
    "\n",
    "            now = time.time()\n",
    "            print ('%f: Worker %d: traing step %d dome (global step:%d)' % (now, FLAGS.task_index, local_step, step))\n",
    "\n",
    "            if step >= FLAGS.train_steps:\n",
    "                break\n",
    "\n",
    "        time_end = time.time()\n",
    "        print ('Training ends @ %f' % time_end)\n",
    "        train_time = time_end - time_begin\n",
    "        print ('Training elapsed time:%f s' % train_time)\n",
    "\n",
    "        val_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n",
    "        val_xent = sess.run(cross_entropy, feed_dict=val_feed)\n",
    "        print ('After %d training step(s), validation cross entropy = %g' % (FLAGS.train_steps, val_xent))\n",
    "    sess.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
