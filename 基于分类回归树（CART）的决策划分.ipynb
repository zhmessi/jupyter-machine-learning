{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在数据挖掘中，决策树主要有两种类似：分类树和决策树。分类树的输出是样本的类别，回归树的输出是一个实数。分类和回归树，即CART（Classification And Regression Tree）,最先由Breiman等提出，也属于一类决策树。CART算法由决策树生成和决策树剪枝两部分组成：\n",
    "\n",
    "# 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量的大\n",
    "# 决策树剪枝：用验证数据集对以生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。\n",
    "# CART算法既可以用于创建分类树，也可以用于创建回归树。CART算法的重要特点包含以下三个方面：\n",
    "\n",
    "# 二分(Binary Split)：在每次判断过程中，都是对样本数据进行二分。CART算法是一种二分递归分割技术，把当前样本划分为两个子样本，使得生成的每个非叶子结点都有两个分支，因此CART算法生成的决策树是结构简洁的二叉树。由于CART算法构成的是一个二叉树，它在每一步的决策时只能是“是”或者“否”，即使一个feature有多个取值，也是把数据分为两部分\n",
    "# 单变量分割(Split Based on One Variable)：每次最优划分都是针对单个变量。\n",
    "# 剪枝策略：CART算法的关键点，也是整个Tree-Based算法的关键步骤。剪枝过程特别重要，所以在最优决策树生成过程中占有重要地位。有研究表明，剪枝过程的重要性要比树生成过程更为重要，对于不同的划分标准生成的最大树(Maximum Tree)，在剪枝之后都能够保留最重要的属性划分，差别不大。反而是剪枝方法对于最优树的生成更为关键。\n",
    "# CART树生成就是递归的构建二叉决策树的过程，对回归使用平方误差最小化准则，对于分类树使用基尼指数(Gini index)准则，进行特征选择，生成二叉树。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART算法Python实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treenode import TreeNode\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    " \n",
    "class DecisionTreeCART:\n",
    " \n",
    "    def __init__(self, X, Y):\n",
    "        self.X_train = X\n",
    "        self.Y_train = Y\n",
    "        self.root_node = TreeNode(None, None, None, None, self.X_train, self.Y_train)\n",
    "        self.features = self.get_features(self.X_train)\n",
    "        self.tree_generate(self.root_node)\n",
    " \n",
    "    def get_features(self, X_train_data):\n",
    "        features = dict()\n",
    "        for i in range(len(X_train_data.columns)):\n",
    "            feature = X_train_data.columns[i]\n",
    "            features[feature] = list(X_train_data[feature].value_counts().keys())\n",
    " \n",
    "        return features\n",
    " \n",
    "    def tree_generate(self, tree_node):\n",
    "        X_data = tree_node.X_data\n",
    "        Y_data = tree_node.Y_data\n",
    "        # get all features of the data set\n",
    "        features = list(X_data.columns)\n",
    " \n",
    "        # 如果Y_data中的实例属于同一类，则置为单结点，并将该类作为该结点的类\n",
    "        if len(list(Y_data.value_counts())) == 1:\n",
    "            tree_node.category = Y_data.iloc[0]\n",
    "            tree_node.children = None\n",
    "            return\n",
    " \n",
    "        # 如果特征集为空，则置为单结点，并将Y_data中最大的类作为该结点的类\n",
    "        elif len(features) == 0:\n",
    "            tree_node.category = Y_data.value_counts(ascending=False).keys()[0]\n",
    "            tree_node.children = None\n",
    "            return\n",
    " \n",
    "        # 否则，计算各特征的基尼指数，选择基尼指数最小的特征\n",
    "        else:\n",
    "            # gini_d = self.compute_gini(Y_data)\n",
    "            XY_data = pd.concat([X_data, Y_data], axis=1)\n",
    "            d_nums = XY_data.shape[0]\n",
    "            min_gini_index = 1\n",
    "            feature = None\n",
    "            feature_value = None\n",
    " \n",
    "            for i in range(len(features)):\n",
    "                # 当前特征有哪些取值\n",
    "                # v = self.features.get(features[i])\n",
    "                v = XY_data[features[i]].value_counts().keys()\n",
    "                # 当前特征的取值只有一种\n",
    "                if len(v) <= 1:\n",
    "                    continue\n",
    "                # 当前特征的每一个取值分为是和不是两类\n",
    "                for j in v:\n",
    "                    Gini_index = 0\n",
    "                    dv = XY_data[XY_data[features[i]] == j]\n",
    "                    dv_nums = dv.shape[0]\n",
    "                    dv_not = XY_data[XY_data[features[i]] != j]\n",
    "                    dv_not_nums = dv_not.shape[0]\n",
    "                    gini_dv = self.compute_gini(dv[dv.columns[-1]])\n",
    "                    gini_dv_not = self.compute_gini(dv_not[dv_not.columns[-1]])\n",
    "                    if d_nums == 0:\n",
    "                        continue\n",
    "                    Gini_index += dv_nums / d_nums * gini_dv + dv_not_nums / d_nums * gini_dv_not\n",
    " \n",
    "                    if Gini_index < min_gini_index:\n",
    "                        min_gini_index = Gini_index\n",
    "                        feature = features[i]\n",
    "                        feature_value = j\n",
    " \n",
    "            if feature is None:\n",
    "                tree_node.category = Y_data.value_counts(ascending=False).keys()[0]\n",
    "                tree_node.children = None\n",
    "                return\n",
    "            tree_node.feature = feature\n",
    " \n",
    "            # 否则，对当前特征的最小基尼指数取值，将Y_data分成两类子集，构建子结点\n",
    "            # get all kinds of values of the current partition feature\n",
    "            # branches = list({feature_value, \"!\"+str(feature_value)})\n",
    "            # branches = list(XY_data[feature].value_counts().keys())\n",
    "            tree_node.children = dict()\n",
    "            for i in range(2):\n",
    "                # 左分支，左分支为是的分支\n",
    "                if i == 0:\n",
    "                    X_data = XY_data[XY_data[feature] == feature_value]\n",
    "                    X_data.drop(feature, axis=1, inplace=True)\n",
    "                    child_name = feature_value\n",
    "                # 右分支，右分支为否的分支\n",
    "                else:\n",
    "                    X_data = XY_data[XY_data[feature] != feature_value]\n",
    "                    child_name = \"!\" + str(feature_value)\n",
    "                # 可能出现节点没有数据的情况吗？\n",
    "                # if len(X_data) == 0:\n",
    "                #     print(\"I'm a bug\")\n",
    "                #     category = XY_data[XY_data.columns[-1]].value_counts(ascending=False).keys()[0]\n",
    "                #     childNode = TreeNode(tree_node, None, None, category, None, None)\n",
    "                #     tree_node.children[child_name] = childNode\n",
    "                #     # return\n",
    "                #     # error, not should return, but continue\n",
    "                #     continue\n",
    " \n",
    "                Y_data = X_data[X_data.columns[-1]]\n",
    "                X_data.drop(X_data.columns[-1], axis=1, inplace=True)\n",
    "                # X_data.drop(feature, axis=1, inplace=True)\n",
    "                childNode = TreeNode(tree_node, None, None, None, X_data, Y_data)\n",
    "                tree_node.children[child_name] = childNode\n",
    "                # print(\"feature: \" + str(tree_node.feature) + \" branch: \" + str(branches[i]) + \"\\n\")\n",
    "                self.tree_generate(childNode)\n",
    " \n",
    "            return\n",
    " \n",
    "    def compute_gini(self, Y):\n",
    "        gini = 1\n",
    "        for cate in Y.value_counts(1):\n",
    "            gini -= cate*cate\n",
    "        return gini"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
