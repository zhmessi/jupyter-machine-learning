{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林是包含多个决策树的分类器。随机森林算法是由 Leo Breiman和Adele Cutler发展推论出的。随机森林，顾名思义就是用随机的方式建立一个森林，森林里面由很多的决策树组成，且这些决策树之间没有关联。\n",
    "\n",
    "# 随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树，而它的本质属于机器学习的一个分支—集成学习(Ensemble Learning)方法。集成学习就是使用一系列学习器进行学习，并将各个学习方法通过某种特定的规则进行整合，以获得比单个学习器更好的学习效果的一种机器学习方法。集成学习通过建立几个模型，并将它们组合来解决单一预测问题。它的工作原理主要是生成多个分类器或模型，各自独立的学习和做出预测。\n",
    "\n",
    "# 随机森林是由多棵决策树构成的。对于每棵树，它们使用的训练集是采用放回的方式从总的训练集中采样出来的。而在训练每棵树的节点时，使用特征是从所有特征中，采用按照一定比例随机的无回放的方式抽取的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林的优点\n",
    "\n",
    "# 随机森林算法能解决分类与回归两种类型的问题，并在这两个方面都有相当好的估计表现；\n",
    "# 随机森林对于高维数据集的处理能力令人兴奋，它可以处理成千上万的输入变量，并确定最重要的变量，因此被认为是一个不错的降维方法。此外，该模型能够输出变量的重要性程度，这是一个非常便利的功能。\n",
    "# 在对缺失数据进行估计时，随机森林是一个十分有效的方法。就算存在大量的数据缺失，随机森林也能较好地保持精确性。\n",
    "# 当存在分类不平衡的情况时，随机森林能够提供平衡数据集误差的有效方法。\n",
    "# 模型的上述性能可以被扩展运用到未标记的数据集中，用于引导无监督聚类、数据透视和异常检测；\n",
    "# 随机森林算法中包含了对输入数据的重复自抽样过程，即所谓的bootstrap抽样。这样一来，数据集中大约三分之一将没有用于模型的训练而是用于测试，这样的数据被称为out of bag samples，通过这些样本估计的误差被称为out of bag error。\n",
    "# 研究表明，这种out of bag方法的与测试集规模同训练集一致的估计方法有着相同的精确程度，因此在随机森林中我们无需再对测试集进行另外的设置。\n",
    "# 训练速度快，容易做成并行化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林的缺点\n",
    "\n",
    "# 随机森林在解决回归问题时并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续型的输出。当进行回归时，随机森林不能够作出超越训练集数据范围的预测，这可能导致在对某些还有特定噪声的数据进行建模时出现过度拟合。\n",
    "# 对于许多统计建模者来说，随机森林给人的感觉像是一个黑盒子——你几乎无法控制模型内部的运行，只能在不同的参数和随机种子之间进行尝试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林的构造方法\n",
    "\n",
    "# 随机森林的建立基本由随机采样和完全分裂两部分组成。\n",
    "\n",
    "# 1、随机采样\n",
    "\n",
    "# 随机森林对输入的数据进行行、列的采样，但两个采样的方法有所不同。对于行采样，采用的方法是有回放的采样，即在采样得到的样本集合中，可能会有重复的样本。假设输入样本为N个，那么采样的样本也是N个，这样使得在训练时，每科树的输入样本都不是全部的样本，所以相对不容易出现over-fitting。对于列采样，采用的方式是按照一定的比例无放回的抽取，从M个feature中，选择m个样本（n<<M）。\n",
    "\n",
    "# 2、完全分裂\n",
    "\n",
    "# 在形成决策树的过程中，决策树的每个节点都要按完全分裂的方式来分裂，直到节点不能再分裂。采用这种方式建立出的决策树的某一叶子节点要么是无法继续分裂的，要么里面的所有样本都是指向同一个分类。\n",
    "\n",
    "# 接下来，将介绍每科树的构造方法，步骤如下：\n",
    "\n",
    "# 用N表示训练集的个数，M表示变量的数目\n",
    "# 用m来表示当在一个节点上做决定时会用到的变量的数量\n",
    "# 从N个训练案例中采用可重复取样的方式，取样N次，形成一组训练集，并使用这棵树来对剩余变量预测其类别，并对误差进行计算。\n",
    "# 对于每个节点，随机选择m个基于词典上的变量。根据这m个变量，计算其最佳的分割方式。\n",
    "# 对于森林中的每棵树都不用采用剪枝技术，每棵树都能完整生长。\n",
    "# 森林中任意两棵的相关性与森林中棵树的分类能力是影响随机森林分类效果（误差率）的两个重要因素。任意两棵树之间的相关性越大，错误率越大，每棵树的分类能力越强，整个森林的错误率越低。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
